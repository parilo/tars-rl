# Env
env:
  #name: swimmer
  #obs_size: 8
  #action_size: 2
  #name: half_cheetah
  #name: walker2d
  #obs_size: 17
  #action_size: 6
  #name: ant
  #obs_size: 111
  #action_size: 8
  #name: humanoid
  #obs_size: 376
  #action_size: 17
  name: bipedal_walker
  obs_size: 24
  action_size: 4
  frame_skip: 1
  agent_buffer_size: 2001

# Networks
actor:
  hiddens: [[256], [256]]
  layer_norm: false
  noisy_layer: false
  activations: [relu, relu]
  output_activation: tanh
  
critic_v:
  hiddens: [[256], [256]]
  layer_norm: false
  noisy_layer: false
  activations: [relu, relu]
  output_activation: null
  action_insert_block: -1

critic_q:
  hiddens: [[256], [256]]
  layer_norm: false
  noisy_layer: false
  activations: [relu, relu]
  output_activation: null
  action_insert_block: 0

actor_optim:
  lr: 0.0003

critic_optim:
  lr: 0.0003

# Algorithm
exploration:
  norm_eps: 0.15
  grad_eps: 0.1

algorithm:
  n_step: 1
  actor_grad_val_clip: 1.0
  gamma: 0.99
  target_critic_update_rate: 0.005
  reward_scale: 50.0

server:
  num_clients: 10
  batch_size: 256
  experience_replay_buffer_size: 1000000
  use_prioritized_buffer: false
  use_synchronous_update: true
  train_every_nth: 1
  history_length: 1
  start_learning_after: 8000
  target_critic_update_period: 1
  target_actor_update_period: 1
  show_stats_period: 100
  save_model_period: 10000
  init_port: 4940
