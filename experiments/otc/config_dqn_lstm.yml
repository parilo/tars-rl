framework: tensorflow

env:
  env_module: envs.otc
  env_class: ObstacleTowerEnvWrapper
  additional_env_parameters:
    environment_filename: "/home/anton/devel/otc/ObstacleTower/obstacletower.x86_64"
    retro: True
#    id_parameter: "worker_id"
  is_gym: False
  obs_shapes: [[1, 84, 84]]  # list of lists
  obs_dtypes: ["uint8"]
  obs_is_image: True
#  obs_image_resize_to: [84, 84]
  obs_image_to_grayscale: True  # used V from HSV
  action_size: 10
  action_postprocess:
    type: "argmax of softmax"
  action_remap_discrete: [
    0,  # no
    3,  # j
    6,  # cam left
    12,  # cam right
    18,  # forward
    21,  # jump + forward
    24,  # forw + cam left
    27,  # f + j + cam left
    30,  # f + cam right
    33   # f + j + cam right
  ]
  frame_skip: 1
  step_limit: 0
  agent_buffer_size: 10001
  history_length: 64
  reward_scale: 1.
  reinit_random_action_every: 5
  log_every_n_steps: 1000
  max_episode_length: 10000
  discrete_actions: True
  reward_clip:
    min: 0.95  # if reward > 0., for example turn 0.1 -> 0.5
#  stop_if_same_state_repeat: 30
  render_with_cv2: True
  render_with_cv2_resize: [400, 400]

server:
  seed: 42
  num_clients: 16
  experience_replay_buffer_size: 4000000
  use_prioritized_buffer: false
  train_every_nth: 1.
  start_learning_after: 5000
  target_critic_update_period: 1
#  target_critic_update_period: 2500
  show_stats_period: 10
  save_model_period: 5000
  ip_address: "0.0.0.0"
  client_start_port: 11977
  logdir: "logs/otc_15"
#  load_checkpoint: "logs/otc_8/model-115000.ckpt"

algo_name: "dqn"

algorithm:
  n_step: 1
  gamma: 0.996
#  critic_grad_val_clip: 1.
  target_critic_update_rate: 0.001
#  target_critic_update_rate: 1.

critic:
  nn_engine: "keras"
  nn_arch:
    layers:
      - type: "Permute"
        args:
          dims: [2, 1, 3, 4]

      - type: "Conv3D"
        args:
          filters: 32
          kernel_size: [1, 8, 8]
          strides: [1, 4, 4]
          padding: "same"
          data_format: "channels_first"
      - type: "Activation"
        args:
          activation: "relu"

      - type: "Conv3D"
        args:
          filters: 64
          kernel_size: [1, 4, 4]
          strides: [1, 2, 2]
          padding: "same"
          data_format: "channels_first"
      - type: "Activation"
        args:
          activation: "relu"

      - type: "Conv3D"
        args:
          filters: 128
          kernel_size: [1, 4, 4]
          strides: [1, 2, 2]
          padding: "same"
          data_format: "channels_first"
      - type: "Activation"
        args:
          activation: "relu"

#      - type: "Conv3D"
#        args:
#          filters: 128
#          kernel_size: [1, 3, 3]
#          strides: [1, 1, 1]
#          padding: "same"
#          data_format: "channels_first"
#      - type: "Activation"
#        args:
#          activation: "relu"

      - type: "Permute"
        args:
          dims: [2, 1, 3, 4]

      - type: "Reshape"
        args:
#          target_shape: [64, 7744]
          target_shape: [64, 4608]

#      - type: "Dense"
#        args:
#          units: 1024
#          activation: "relu"
#
#      - type: "Dense"
#        args:
#          units: 512
#          activation: "relu"

#      - type: "Dense"
#        args:
#          units: 1024
#          activation: "relu"
#
#      - type: "Dense"
#        args:
#          units: 512
#          activation: "relu"

      - type: "CuDNNLSTM"
        args:
          units: 256
#          return_sequences: True

      - type: "Dense"
        args:
          units: 512
          activation: "relu"

      - type: "Dense"
        args:
          units: 512
          activation: "relu"

      - type: "Dense"
        args:
          units: 10  # number of actions
          activation: "linear"
          kernel_initializer:
            class: "RandomUniform"
            module: "tensorflow.python.keras.initializers"
            args:
              minval: -0.003
              maxval: 0.003
          bias_initializer:
            class: "RandomUniform"
            module: "tensorflow.python.keras.initializers"
            args:
              minval: -0.003
              maxval: 0.003

critic_optim:
  schedule:
    - limit: 0  # of train ops
      lr: 0.0001

training:
  schedule:
    - limit: 0  # of train ops
      batch_size: 32

agents:
  - algorithm_id: 0
    agents:
      - agents_count: 1
#        store_episodes: True
        visualize: True
#        env_params:
#          start_level_inc: -1  # from finished

      - agents_count: 1
#        store_episodes: True
#        visualize: True
        exploration:
          type: "boltzmann"
          temp: 0.05
#        env_params:
#          start_level_inc: -1

      - agents_count: 1
#        store_episodes: True
        exploration:
          type: "boltzmann"
          temp: 0.05
        env_params:
          start_level_inc: -1

      - agents_count: 1
#        store_episodes: True
        exploration:
          type: "boltzmann"
          temp: 0.05
        env_params:
          start_level_inc: -1
