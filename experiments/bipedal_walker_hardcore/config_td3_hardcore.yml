framework: tensorflow

env:
  name: BipedalWalkerHardcore-v2
  is_gym: True
  obs_size: 24
  action_size: 4
  frame_skip: 8
  step_limit: 0
  agent_buffer_size: 510
  history_length: 3
  reward_scale: 0.1
  reinit_random_action_every: 5

server:
  seed: 42
  num_clients: 16
  experience_replay_buffer_size: 5000000
  use_prioritized_buffer: false
  train_every_nth: 1.
  start_learning_after: 5000
  target_critic_update_period: 1
  target_actor_update_period: 1
  show_stats_period: 100
  save_model_period: 5000
  client_start_port: 10977
  logdir: "logs/bipedal_walker_hardcore_td3"
  #load_checkpoint: ""

algo_name: "td3"

algorithm:
  n_step: 1
  gamma: 0.99
  actor_grad_val_clip: 1.0
  target_actor_update_rate: 0.0025
  target_critic_update_rate: 0.005

actor:
  lstm_network: False
  hiddens: [128, 128, 128]
  activations: ["relu", "relu", "relu"]
  layer_norm: False
  noisy_layer: False
  output_activation: "tanh"

critic:
  lstm_network: False
  hiddens: [128, 128, 128]
  activations: ["relu", "relu", "relu"]
  layer_norm: False
  noisy_layer: False
  output_activation: "linear"
  action_insert_block: 0
  num_atoms: 1

actor_optim:
  schedule:
    - limit: 0  # of train ops
      lr: 0.001
    - limit: 500000
      lr: 0.0005
    - limit: 1000000
      lr: 0.0005
    - limit: 1500000
      lr: 0.00025

critic_optim:
  schedule:
    - limit: 0  # of train ops
      lr: 0.001
    - limit: 500000
      lr: 0.0005
    - limit: 1000000
      lr: 0.0005
    - limit: 1500000
      lr: 0.00025

training:
  schedule:
    - limit: 0  # of train ops
      batch_size: 2560
#    - limit: 500000
#      batch_size: 2560
#    - limit: 1000000
#      batch_size: 5120
#    - limit: 1500000
#      batch_size: 5120

agents:
  - algorithm_id: 0
    agents:
      - agents_count: 1
        visualize: True
        #repeat_action: 1

      - agents_count: 1
        visualize: False
        #repeat_action: 1

      - agents_count: 1
        visualize: True
        #repeat_action: 4
        #random_repeat_action: True
        exploration:
          normal_noise: 0.2
          random_action_prob: 0.1

      - agents_count: 1
        visualize: False
        #repeat_action: 4
        #random_repeat_action: True
        exploration:
          normal_noise: 0.1
          random_action_prob: 0.1

      - agents_count: 1
        visualize: False
        #repeat_action: 4
        #random_repeat_action: True
        exploration:
          normal_noise: 0.2
          random_action_prob: 0.1

      - agents_count: 1
        visualize: False
        #repeat_action: 4
        #random_repeat_action: True
        exploration:
          normal_noise: 0.2
          random_action_prob: 0.2

      - agents_count: 1
        visualize: False
        #repeat_action: 4
        #random_repeat_action: True
        exploration:
          normal_noise: 0.5
          random_action_prob: 0.2
