framework: torch
device: 'cuda:0'

env:
  name: Pendulum-v0
  is_gym: True

  obs_shapes: [[3]]
  obs_dtypes: ["float32"]
  obs_is_image: False
  history_length: [1]

  action_size: 1
  frame_skip: 2
  step_limit: 0
  agent_buffer_size: 110
  reward_scale: 1
  reinit_random_action_every: 5
  log_every_n_steps: 1000
  max_episode_length: 110

server:
  seed: 42
  num_clients: 16
  experience_replay_buffer_size: 5000000
  use_prioritized_buffer: false
  train_history_len: [8]
  action_history: True
  train_every_nth: 1
  start_learning_after: 5000
  target_critic_update_period: 1
  target_actor_update_period: 1
  show_stats_period: 1
  save_model_period: 5000
  client_start_port: 10977
  logdir: "logs/pendulum_el_torch"
  #load_checkpoint: ""

algo_name: "env_learning"

eml_algorithm:
  update_inner_algo_every: 2
  num_gen_episodes: 200
  num_env_steps: 101
  env_train_start_step_count: 200
  env_funcs_module: envs.funcs.pendulum
  is_done_func: is_done
  calc_reward_func: calc_reward

# inner algorithm
algorithm:
  number_elite_episodes: 20
  transitions_buffer_size: 10000
  sigma: 0.1
  tanh_limit:
    min: -2
    max: 2

env_model:
  have_action_input: true
  nn_arch:

    - input: 0
      name: "flatten-obs"
      layers:
        - type: "Flatten"

    - input: ["flatten-obs", "action"]
      name: "output"
      layers:

        - type: cat
          module: torch
          is_func: True
          args:
            dim: 1

        - type: "Linear"
          args:
            in_features: 4
            out_features: 128

        - type: "ReLU"

        - type: "Linear"
          args:
            in_features: 128
            out_features: 128

        - type: "ReLU"
#
#        - type: "Linear"
#          args:
#            in_features: 128
#            out_features: 128

#        - type: "ReLU"

        - type: "Linear"
          args:
            in_features: 128
            out_features: 3

        - type: 'Reshape'
          args:
            shape: [1, 3]

actor:
  nn_arch:

    - input: 0
      name: "output"
      layers:
        - type: "Flatten"

        - type: "Linear"
          args:
            in_features: 3
            out_features: 128

        - type: "ReLU"

        - type: "Linear"
          args:
            in_features: 128
            out_features: 128

        - type: "ReLU"

        - type: "Linear"
          args:
            in_features: 128
            out_features: 128

        - type: "ReLU"

        - type: "Linear"
          args:
            in_features: 128
            out_features: 1

optim:
  module: torch.optim
  class: Adam
  schedule:
    - limit: 0  # of train ops
      lr: 0.0001

actor_optim:
  module: torch.optim
  class: Adam
  schedule:
    - limit: 0  # of train ops
      lr: 0.001

training:
  schedule:
    - limit: 0  # of train ops
      batch_size: 2560

agents:
  - algorithm_id: 0
    agents:
      - agents_count: 1
        visualize: True
        exploration:
          built_in_algo: True
          validation: True

      - agents_count: 1
        visualize: False
        exploration:
          built_in_algo: True
          validation: True

      - agents_count: 1
        visualize: False
        exploration:
          built_in_algo: True
          validation: False
