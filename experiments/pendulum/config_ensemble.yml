# example of "one-batch" ensemble of algorithms
# they uses the same batch for training

env:
  name: Pendulum-v0
  is_gym: True
  obs_size: 3
  action_size: 1
  frame_skip: 2
  step_limit: 0
  agent_buffer_size: 110
  history_length: 2
  reward_scale: 0.1
  reinit_random_action_every: 5
  remap_action:
    low:
      before: -1.
      after: -2.
    high:
      before: 1.
      after: 2.

server:
  seed: 42
  num_clients: 16
  experience_replay_buffer_size: 5000000
  use_synchronous_update: false
  train_every_nth: 1.
  start_learning_after: 5000
  target_critic_update_period: 1
  target_actor_update_period: 1
  show_stats_period: 100
  save_model_period: 25000
  client_start_port: 10977
  logdir: "logs/asd2"
  #load_checkpoint: ""

# one-batch algorithms in ensamble shares
# n_step and gamma parameters since they
# uses the same batch for training
# also keep same parameters inside ensemble.algorithms
algorithm:
  n_step: 1
  gamma: 0.99

ensemble:
  type: "one-batch"
  algorithms:
    - algorithm_id: 0
      framework: tensorflow
      algo_name: "quantile_td3"

      algorithm:
        n_step: 1
        gamma: 0.99
        actor_grad_val_clip: 1.0
        target_actor_update_rate: 0.0025
        target_critic_update_rate: 0.005

      use_lstm_networks: False

      actor:
        hiddens: [[128, 128, 128]]
        layer_norm: False
        noisy_layer: False
        activations: ["relu"]
        output_activation: "tanh"

      critic:
        hiddens: [[128, 128]]
        layer_norm: False
        noisy_layer: False
        activations: ["relu"]
        output_activation: "linear"
        action_insert_block: 0
        num_atoms: 128

      actor_optim:
        schedule:
          - limit: 0  # of train ops
            lr: 0.001
          - limit: 500000
            lr: 0.0005
          - limit: 1000000
            lr: 0.0005
          - limit: 1500000
            lr: 0.00025

      critic_optim:
        schedule:
          - limit: 0  # of train ops
            lr: 0.001
          - limit: 500000
            lr: 0.0005
          - limit: 1000000
            lr: 0.0005
          - limit: 1500000
            lr: 0.00025

      training:
        schedule:
          - limit: 0  # of train ops
            batch_size: 2560
          - limit: 500000
            batch_size: 2560
          - limit: 1000000
            batch_size: 5120
          - limit: 1500000
            batch_size: 5120

    - algorithm_id: 1
      framework: tensorflow
      algo_name: "ddpg"

      algorithm:
        n_step: 1
        gamma: 0.99
        actor_grad_val_clip: 1.0
        target_actor_update_rate: 0.0025
        target_critic_update_rate: 0.005

      use_lstm_networks: False

      actor:
        hiddens: [[128, 128, 128]]
        layer_norm: False
        noisy_layer: False
        activations: ["relu"]
        output_activation: "tanh"

      critic:
        hiddens: [[128, 128]]
        layer_norm: False
        noisy_layer: False
        activations: ["relu"]
        output_activation: "linear"
        action_insert_block: 0
        num_atoms: 128

      actor_optim:
        schedule:
          - limit: 0  # of train ops
            lr: 0.001
          - limit: 500000
            lr: 0.0005
          - limit: 1000000
            lr: 0.0005
          - limit: 1500000
            lr: 0.00025

      critic_optim:
        schedule:
          - limit: 0  # of train ops
            lr: 0.001
          - limit: 500000
            lr: 0.0005
          - limit: 1000000
            lr: 0.0005
          - limit: 1500000
            lr: 0.00025

      training:
        schedule:
          - limit: 0  # of train ops
            batch_size: 2560
          - limit: 500000
            batch_size: 2560
          - limit: 1000000
            batch_size: 5120
          - limit: 1500000
            batch_size: 5120

agents:
  - algorithm_id: 0
    agents:
      - agents_count: 1
        visualize: True

      - agents_count: 1
        visualize: True
        exploration:
          normal_noise: 0.5
          random_action_prob: 0.2

      - agents_count: 2
        visualize: False
        exploration:
          normal_noise: 0.5
          random_action_prob: 0.2

  - algorithm_id: 1
    agents:
      - agents_count: 1
        visualize: True

      - agents_count: 1
        visualize: True
        exploration:
          normal_noise: 0.5
          random_action_prob: 0.2

      - agents_count: 2
        visualize: False
        exploration:
          normal_noise: 0.5
          random_action_prob: 0.2
